{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coregistration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKt-HqjwegU",
        "colab_type": "text"
      },
      "source": [
        "Run this notebook in Google Colab.\n",
        "Parts of the code use GPU so go to Runtime -> Change Runtime type - > GPU. \n",
        "# Initialize:\n",
        "This code will prompt you to give access to your google drive, where all the files should be stored. All the file paths should start with \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wPuL39_kDQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "4f4165b8-bf87-4167-e09b-e0d633708f90"
      },
      "source": [
        "import scipy as sp\n",
        "import scipy.misc\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%matplotlib inline\n",
        "!nvidia-smi\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "from scipy.special import expit\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from scipy.optimize import fmin, fmin_tnc\n",
        "import math\n",
        "from skimage.filters import threshold_minimum, median\n",
        "from skimage.morphology import disk\n",
        "from random import  uniform\n",
        "from os import listdir\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from IPython.display import clear_output\n",
        "def calc_iou(prediction, ground_truth):\n",
        "    n_images = len(prediction)\n",
        "    intersection, union = 0, 0\n",
        "    for i in range(n_images):\n",
        "        intersection += np.logical_and(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum() \n",
        "        union += np.logical_or(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum()\n",
        "    return float(intersection) / union\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "      \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "      \n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, X, transform_X=transforms.ToTensor()):\n",
        "        self.X = X\n",
        "        self.transform_X = transform_X\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.X[index]\n",
        "        x = self.transform_X(x)\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "   \n",
        "def matchshape(confocal,maldi):\n",
        "    # we assume here confocal has a higher resolution than maldi\n",
        "    # we reduce the resolution of outlines so the alignment works faster\n",
        "    h1, w1 = confocal.shape\n",
        "    h2, w2 = maldi.shape\n",
        "    common_denominator = h1/512\n",
        "    maldi = cv2.resize(maldi, dsize=(int(w2*h1/(h2*common_denominator)),int(h1/common_denominator)), interpolation=cv2.INTER_CUBIC)\n",
        "    confocal = cv2.resize(confocal, dsize=(int(w1/common_denominator),int(h1/common_denominator)), interpolation=cv2.INTER_CUBIC)\n",
        "    maldi_denominator = h1/h2\n",
        "    h1, w1 = confocal.shape\n",
        "    h2, w2 = maldi.shape\n",
        "    output1 = np.zeros((max(h1,h2),max(w1,w2)))\n",
        "    output2 = np.zeros((max(h1, h2), max(w1, w2)))\n",
        "    for i in range(len(output1)):\n",
        "        try:\n",
        "            output1[i][:len(confocal[i])] = confocal[i]\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            output2[i][:len(maldi[i])] = maldi[i]\n",
        "        except:\n",
        "            pass\n",
        "    return output1.astype(np.uint8),output2.astype(np.uint8), maldi_denominator, common_denominator\n",
        "\n",
        "def transform_image(X,theta,shiftX,shiftY,scale): # error - negative scale\n",
        "    (height, width) = X.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((width/2,height/2), np.degrees(theta), 1)\n",
        "    Y = cv2.warpAffine(X, M, (width,height))\n",
        "    Y = cv2.resize(Y, dsize=(int(width*scale),int(height*scale)), interpolation=cv2.INTER_CUBIC)\n",
        "    moveX, moveY = width*(scale-1)/2,height*(scale-1)/2\n",
        "    M = np.float32([[1, 0, shiftX-moveX], [0, 1, shiftY-moveY]])\n",
        "    res = cv2.warpAffine(Y, M, (int(width*scale),int(height*scale)))\n",
        "    res = cv2.warpAffine(Y, M, (width,height))\n",
        "    return res\n",
        "\n",
        "def error_fun2(p,X,Y):\n",
        "    \"\"\"cost function to minimize \"\"\"\n",
        "    if p[3] < 0.01: return X.shape[0]**2\n",
        "    transformed_Y = transform_image(Y,p[0],p[1],p[2],p[3])\n",
        "    out = np.abs(np.subtract(transformed_Y,X))\n",
        "    return np.sum(out)\n",
        "\n",
        " \n",
        "def cut_background(image):\n",
        "  shape = image.shape\n",
        "  top,bottom,left,right = 0,shape[0],0,shape[1]\n",
        "  for i in range(shape[0]):\n",
        "    s = np.sum(image[i,:])\n",
        "    if s >= 1:\n",
        "      bottom = i\n",
        "      if top == 0: top = i\n",
        "  for i in range(shape[1]):\n",
        "    s = np.sum(image[:,i])\n",
        "    if s >= 1:\n",
        "      right = i\n",
        "      if left == 0: left = i   \n",
        "  return image[top:bottom,left:right],[top,bottom,left,right]\n",
        "\n",
        "def pad_zeros(image, scale, gap = 0):\n",
        "  h, w = image.shape[:2]\n",
        "  vertical_zeros = np.zeros((h,int(w*(scale-1)/2)))\n",
        "  if gap == 0:\n",
        "    image = np.concatenate((vertical_zeros,image,vertical_zeros),axis = 1)\n",
        "  else:\n",
        "    image = np.concatenate((vertical_zeros,image,np.zeros((h,int(w*(scale-1)/2)+gap))),axis = 1)\n",
        "  horizontal_zeros = np.zeros((int(h*(scale-1)/2),image.shape[1]))\n",
        "  image = np.concatenate((horizontal_zeros,image,horizontal_zeros),axis = 0)\n",
        "\n",
        "  return image"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Thu Oct  3 13:52:45 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sMQoWmxZ5w1",
        "colab_type": "text"
      },
      "source": [
        "Run this next cell to get the confocal image oulines for aligning. \"tensor.pt\" file is provided in the same github folder with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ofkpfq_kUIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confocal_filename = '/content/drive/My Drive/confocal for segmentation/colonies/test/day 4_box5_0.bmp'\n",
        "model_filename = '/content/drive/My Drive/confocal for segmentation/tensor.pt'\n",
        "net = UNet(n_channels=3, n_classes=1).cuda()\n",
        "net.load_state_dict(torch.load(model_filename))\n",
        "confocal = Image.open(confocal_filename).convert('RGB')\n",
        "w, h = confocal.size\n",
        "confocal2 = confocal.resize((512,512), Image.ANTIALIAS)\n",
        "test_dataset = AugmentedDataset([confocal2])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "Y_pred = []\n",
        "for X_batch in test_dataloader:\n",
        "    Y_pred.append(expit(net(X_batch.cuda()).data.cpu().numpy()) > 0.33)\n",
        "Y_pred = np.concatenate(Y_pred)\n",
        "confocal_outline = Image.fromarray(np.uint8(Y_pred[0, 0, :, :] * 255) , 'L')\n",
        "#display(confocal2,confocal_outline) # uncomment to see the confocal outline created\n",
        "confocal_outline = np.array(confocal_outline.resize((w,h), Image.ANTIALIAS))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY5XRA8VauNJ",
        "colab_type": "text"
      },
      "source": [
        "Run this cell if you already have an outline of the confocal image:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yn3l8WPa2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confocal_outline_path = \"/content/drive/My Drive/confocal for segmentation/outline.png\"\n",
        "confocal_outline = cv2.imread(confocal_outline_path, cv2.IMREAD_GRAYSCALE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kInRvMOrcawJ",
        "colab_type": "text"
      },
      "source": [
        "Run this cell to get an outline of the maldi for the alignment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr3bGcT5aOyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maldi_for_outline = \"/content/drive/My Drive/confocal for segmentation/maldi folder/MALDI__887.553 (1).png\"\n",
        "imMaldi = cv2.imread(maldi_for_outline, cv2.IMREAD_GRAYSCALE)\n",
        "h2, w2 = imMaldi.shape # initial dimentions\n",
        "t = threshold_minimum(imMaldi)\n",
        "imMaldi = imMaldi > t\n",
        "maldi_outline = 1-imMaldi.astype(np.uint8)\n",
        "maldi_outline = median(maldi_outline, disk(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDLjrMFIcj93",
        "colab_type": "text"
      },
      "source": [
        "Run this cell if you already have an outline of the maldi:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9x7XR8KcjUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maldi_outline_path = \"/content/drive/My Drive/confocal for segmentation/outline.png\"\n",
        "maldi_outline = cv2.imread(maldi_outline_path, cv2.IMREAD_GRAYSCALE)\n",
        "maldi_outline = maldi_outline/np.max(maldi_outline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4jFbFPWc_oq",
        "colab_type": "text"
      },
      "source": [
        "## Start alignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1QKehF-vC-2",
        "colab_type": "code",
        "outputId": "22e666ef-e369-453d-b44d-ef73b662ae3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "confocal_outline2 = 1-np.array(confocal_outline)/255\n",
        "confocal_outline2, maldi_outline, maldi_denominator, common_denominator = matchshape(confocal_outline2,maldi_outline)\n",
        "h3, w3 = confocal_outline2.shape #common dimentions\n",
        "added_image = cv2.addWeighted(maldi_outline,1,confocal_outline2,1,0)\n",
        "mn = sys.maxsize\n",
        "iterations = 50 # increase for a better alignment, decrease for a faster\n",
        "flag = 0\n",
        "########\n",
        "mn = sys.maxsize\n",
        "for i in range(iterations):\n",
        "  scale = uniform(1,2.5)\n",
        "  p_opt = fmin(error_fun2, [uniform(-0.1, 0.1),uniform(-100, 100),uniform(-100, 100),scale], args=(maldi_outline.astype(np.float64),confocal_outline2.astype(np.float64)),xtol = 1e-13,ftol = 1e-13,disp = 0)\n",
        "  fopt = error_fun2(p_opt,maldi_outline.astype(np.float64),confocal_outline2.astype(np.float64))\n",
        "  if fopt < mn:\n",
        "    mn = fopt\n",
        "    xopt = p_opt\n",
        "  print(\"iteration \"+str(i)+\"/\"+str(2*iterations))\n",
        "for i in range(iterations):\n",
        "  scale = uniform(1,2.5)\n",
        "  p_opt = fmin(error_fun2, [uniform(-0.1, 0.1),uniform(-100, 100),uniform(-100, 100),scale], args=(confocal_outline2.astype(np.float64),maldi_outline.astype(np.float64)),xtol = 1e-13,ftol = 1e-13,disp = 0)\n",
        "  fopt = error_fun2(p_opt,confocal_outline2.astype(np.float64),maldi_outline.astype(np.float64))\n",
        "  if fopt < mn:\n",
        "    mn = fopt\n",
        "    xopt = p_opt\n",
        "    flag = 1\n",
        "  print(\"iteration \"+str(iterations+i)+\"/\"+str(2*iterations))\n",
        "if flag == 1:\n",
        "  maldi = transform_image(maldi_outline,xopt[0],xopt[1],xopt[2],xopt[3])\n",
        "  added_image = cv2.addWeighted(maldi,1,confocal_outline2,1,0)\n",
        "  plt.imshow(added_image, cmap = 'gray')\n",
        "  plt.show()\n",
        "  print(\"Optimal parameters (transform maldi): rotation \",xopt[0],\", X shift \",xopt[1],\",Y shift \",xopt[2],\", scale \",xopt[3]) \n",
        "else:\n",
        "  print(\"Optimal parameters (transform confocal): rotation \",xopt[0],\", X shift \",xopt[1],\",Y shift \",xopt[2],\", scale \",xopt[3]) \n",
        "  maldi = transform_image(maldi_outline,-xopt[0],-xopt[1]/xopt[3],-xopt[2]/xopt[3],1/xopt[3])\n",
        "  added_image = cv2.addWeighted(maldi,1,confocal_outline2,1,0)\n",
        "  plt.imshow(added_image, cmap = 'gray')\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0/50\n",
            "iteration 1/50\n",
            "iteration 2/50\n",
            "iteration 3/50\n",
            "iteration 4/50\n",
            "iteration 5/50\n",
            "iteration 6/50\n",
            "iteration 7/50\n",
            "iteration 8/50\n",
            "iteration 9/50\n",
            "iteration 10/50\n",
            "iteration 11/50\n",
            "iteration 12/50\n",
            "iteration 13/50\n",
            "iteration 14/50\n",
            "iteration 15/50\n",
            "iteration 16/50\n",
            "iteration 17/50\n",
            "iteration 18/50\n",
            "iteration 19/50\n",
            "iteration 20/50\n",
            "iteration 21/50\n",
            "iteration 22/50\n",
            "iteration 23/50\n",
            "iteration 24/50\n",
            "iteration 25/50\n",
            "iteration 26/50\n",
            "iteration 27/50\n",
            "iteration 28/50\n",
            "iteration 29/50\n",
            "iteration 30/50\n",
            "iteration 31/50\n",
            "iteration 32/50\n",
            "iteration 33/50\n",
            "iteration 34/50\n",
            "iteration 35/50\n",
            "iteration 36/50\n",
            "iteration 37/50\n",
            "iteration 38/50\n",
            "iteration 39/50\n",
            "iteration 40/50\n",
            "iteration 41/50\n",
            "iteration 42/50\n",
            "iteration 43/50\n",
            "iteration 44/50\n",
            "iteration 45/50\n",
            "iteration 46/50\n",
            "iteration 47/50\n",
            "iteration 48/50\n",
            "iteration 49/50\n",
            "iteration 0/50\n",
            "iteration 1/50\n",
            "iteration 2/50\n",
            "iteration 3/50\n",
            "iteration 4/50\n",
            "iteration 5/50\n",
            "iteration 6/50\n",
            "iteration 7/50\n",
            "iteration 8/50\n",
            "iteration 9/50\n",
            "iteration 10/50\n",
            "iteration 11/50\n",
            "iteration 12/50\n",
            "iteration 13/50\n",
            "iteration 14/50\n",
            "iteration 15/50\n",
            "iteration 16/50\n",
            "iteration 17/50\n",
            "iteration 18/50\n",
            "iteration 19/50\n",
            "iteration 20/50\n",
            "iteration 21/50\n",
            "iteration 22/50\n",
            "iteration 23/50\n",
            "iteration 24/50\n",
            "iteration 25/50\n",
            "iteration 26/50\n",
            "iteration 27/50\n",
            "iteration 28/50\n",
            "iteration 29/50\n",
            "iteration 30/50\n",
            "iteration 31/50\n",
            "iteration 32/50\n",
            "iteration 33/50\n",
            "iteration 34/50\n",
            "iteration 35/50\n",
            "iteration 36/50\n",
            "iteration 37/50\n",
            "iteration 38/50\n",
            "iteration 39/50\n",
            "iteration 40/50\n",
            "iteration 41/50\n",
            "iteration 42/50\n",
            "iteration 43/50\n",
            "iteration 44/50\n",
            "iteration 45/50\n",
            "iteration 46/50\n",
            "iteration 47/50\n",
            "iteration 48/50\n",
            "iteration 49/50\n",
            "Optimal parameters (transform confocal): rotation  0.022268003858665397 , X shift  -102.66124352521291 ,Y shift  -15.73527033231966 , scale  1.5579504608178043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAVmiIgTobEt",
        "colab_type": "text"
      },
      "source": [
        "If you already have your alignment parameters input them at the cell below and run it. Flag = 1 means your parameters are for a MALDI transformation, flag = 0 for a confocal image transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW1UA8a7obcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag = 1\n",
        "xopt = [0,0,0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z_D5sHZpQnD",
        "colab_type": "text"
      },
      "source": [
        "## Crop and save the aligned images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_1Ac-164FFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file locations\n",
        "maldi_folder_path = \"/content/drive/My Drive/confocal for segmentation/maldi folder\"\n",
        "maldi_output_path = \"/content/drive/My Drive/confocal for segmentation/maldi aligned\"\n",
        "crop_confocal_path = \"/content/drive/My Drive/confocal for segmentation/cropped_confocal.png\"\n",
        "#####################################################################################\n",
        "if not os.path.exists(maldi_output_path):os.mkdir(maldi_output_path)\n",
        "l = os.listdir(maldi_folder_path)\n",
        "w_conf = int(w/common_denominator)\n",
        "w_maldi = int(w2*maldi_denominator/common_denominator) \n",
        "\n",
        "if flag == 0: # means confocal was scaled up, image1 = confocal\n",
        "  if w3 > w_conf: # image2 (maldi) is wider, image1 has a black gap on the right\n",
        "    image2_outlines = np.ones((h3,w3))\n",
        "    image2_outlines_transformed = transform_image(image2_outlines,-1*xopt[0],-1*xopt[1]/xopt[3],-1*xopt[2]/xopt[3],1/xopt[3]) # here all the xopt were done for confocal, xopt[3]>1, but here we scale maldi down, so we reverse xopt\n",
        "    _, [top,bottom,left,right] = cut_background(image2_outlines_transformed)\n",
        "    if right > w_conf:\n",
        "      right = w_conf # confocal and maldi are going to be cropped right with [top:bottom,left:right] \n",
        "  elif w3 == w_conf: # image1 (confocal) is wider, image2 has a black gap on the right\n",
        "    image2_outlines = np.concatenate((np.ones((h3,w_maldi)),np.zeros((h3,w3-w_maldi))), axis=1)\n",
        "    image2_outlines_transformed = transform_image(image2_outlines,-1*xopt[0],-1*xopt[1]/xopt[3],-1*xopt[2]/xopt[3],1/xopt[3]) \n",
        "    _, [top,bottom,left,right] = cut_background(image2_outlines_transformed)\n",
        "  cropped_confocal = np.array(confocal)[int(common_denominator*top):int(bottom*common_denominator),int(common_denominator*left):int(right*common_denominator)]\n",
        "  for name in l: # make sure you have only maldi images of the same run in the folder, no technical files like \".Thumbs\" etc\n",
        "    im = cv2.imread(maldi_folder_path+\"/\"+name, cv2.IMREAD_GRAYSCALE)\n",
        "    _, im, _, _ = matchshape(confocal_outline2,im)\n",
        "    im = transform_image(im,-1*xopt[0],-1*xopt[1]/xopt[3],-1*xopt[2]/xopt[3],1/xopt[3])\n",
        "    im = im[top:bottom,left:right]\n",
        "    im = cv2.resize(im, dsize=(cropped_confocal.shape[1],cropped_confocal.shape[0]), interpolation=cv2.INTER_CUBIC)     \n",
        "    cv2.imwrite(maldi_output_path+\"/\"+name,im)\n",
        "  cv2.imwrite(crop_confocal_path,cv2.cvtColor(cropped_confocal,cv2.COLOR_RGB2BGR))\n",
        "else: # means maldi was scaled up, image1 = maldi\n",
        "  print(w3,w_conf,w_maldi)\n",
        "  if w3 > w_maldi: # image2 (confocal) is wider, image1 has a black gap on the right\n",
        "    image2_outlines = np.ones((h3,w3))\n",
        "    image2_outlines_transformed = transform_image(image2_outlines,0,-1*xopt[1]/xopt[3],-1*xopt[2]/xopt[3],1/xopt[3]) # here all the xopt were done for maldi, xopt[3]>1, but we pretend we scaled confocal down, so we reverse xopt \n",
        "    _, [top,bottom,left,right] = cut_background(image2_outlines_transformed)\n",
        "    if right > w_maldi:\n",
        "      right = w_maldi\n",
        "    new_confocal = pad_zeros(np.array(confocal)[:,:,2],xopt[3])\n",
        "  elif w3 == w_maldi: # image1 (maldi) is wider, image2 has a black gap on the right\n",
        "    image2_outlines = np.concatenate((np.ones((h3,w_conf)),np.zeros((h3,w3-w_conf))), axis=1)\n",
        "    image2_outlines_transformed = transform_image(image2_outlines,0,-1*xopt[1]/xopt[3],-1*xopt[2]/xopt[3],1/xopt[3]) \n",
        "    plt.imshow(image2_outlines_transformed)\n",
        "    plt.show()\n",
        "    im, [top,bottom,left,right] = cut_background(image2_outlines_transformed)\n",
        "    new_confocal = pad_zeros(np.array(confocal)[:,:,2],xopt[3],gap = int((w3-w_conf)*common_denominator))\n",
        "  new_confocal_transformed = transform_image(new_confocal,0,-common_denominator*xopt[1],-common_denominator*xopt[2],1)\n",
        "  multiplier = new_confocal_transformed.shape[0]/image2_outlines_transformed.shape[0]\n",
        "  cropped_confocal = new_confocal_transformed[int(top*multiplier):int(bottom*multiplier),int(left*multiplier):int(right*multiplier)] \n",
        "  cropped_confocal = np.stack((cropped_confocal,np.zeros(cropped_confocal.shape),np.zeros(cropped_confocal.shape)),axis = -1)\n",
        "  for name in l: # make sure you have only maldi images of the same run in the folder, no technical files like \".Thumbs\" etc\n",
        "    im = cv2.imread(maldi_folder_path+\"/\"+name, cv2.IMREAD_GRAYSCALE)\n",
        "    _, im, _, _ = matchshape(confocal_outline2,im)\n",
        "    im = transform_image(im,xopt[0],0,0,1)\n",
        "    im = im[top:bottom,left:right]\n",
        "    im = cv2.resize(im, dsize=(cropped_confocal.shape[1],cropped_confocal.shape[0]), interpolation=cv2.INTER_CUBIC)     \n",
        "    cv2.imwrite(maldi_output_path+\"/\"+name,im)\n",
        "  cv2.imwrite(crop_confocal_path,cropped_confocal)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}